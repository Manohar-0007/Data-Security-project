# -*- coding: utf-8 -*-
"""Level-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V4D3cZHTt2kzci7URMekpxCv6BXeGocL
"""

pip install torchvision

import torch
import torchvision
import torchvision.transforms as transforms
import numpy as np
from torch.utils.data import random_split, DataLoader
import torch.nn.functional as F
import torch.nn as nn
import copy

device = torch.device(cudaIdx if torch.cuda.is_available() else "cpu")

def load_data(transform, datasets='MNIST'):
    if datasets == 'MNIST':
        train_dataset = torchvision.datasets.MNIST(
            root="./data/mnist", train=True, download=True, transform=transform)
        test_dataset = torchvision.datasets.MNIST(
            root="./data/mnist", train=False, download=True, transform=transform)
    else:
        train_dataset = torchvision.datasets.CIFAR10(
            root="./data/cifar-10-python", train=True, download=True, transform=transform)
        test_dataset = torchvision.datasets.CIFAR10(
            root="./data/cifar-10-python", train=False, download=True, transform=transform)

    return train_dataset, test_dataset

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset, test_dataset = load_data(transform, datasets='MNIST')

print(train_dataset,test_dataset)



"""# Data split among clients"""

def split_dataset(dataset, num_clients=10):
    # Split dataset into `num_clients` partitions
    client_data = random_split(dataset, [len(dataset) // num_clients] * num_clients)
    return client_data

# Split the MNIST dataset into 10 parts
client_data = split_dataset(train_dataset, num_clients=10)

transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize to 224x224 for ResNet
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)),  # Normalize for single-channel images
    transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Repeat the single channel to create 3 channels
])

"""#Level -2 Final"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# Define a simple ResNet-like backbone
class SimpleResNet(nn.Module):
    def __init__(self):
        super(SimpleResNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.layer1 = self._make_layer(16, 32, stride=2)
        self.layer2 = self._make_layer(32, 64, stride=2)

    def _make_layer(self, in_channels, out_channels, stride):
        layers = [
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
        ]
        return nn.Sequential(*layers)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        return x

# Define the hybrid model
# HybridModel with dynamic feature size calculation
class HybridModel(nn.Module):
    def __init__(self):
        super(HybridModel, self).__init__()
        self.feature_extractor = SimpleResNet()

        # Calculate feature dimension dynamically
        dummy_input = torch.randn(1, 1, 28, 28)
        feature_output = self.feature_extractor(dummy_input)
        self.feature_dim = feature_output.shape[1] * feature_output.shape[2] * feature_output.shape[3]

        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 10)  # MNIST has 10 classes
        )

    def forward(self, x):
        x = self.feature_extractor(x)
        x = x.view(x.size(0), -1)  # Flatten feature map
        x = self.classifier(x)
        return x

# Train a model locally on a client's data
def train_local_model(dataset, model, epochs, batch_size, learning_rate):
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()

    model.train()
    for epoch in range(epochs):
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

# Federated aggregation
def federated_aggregate(global_model, client_models):
    global_dict = global_model.state_dict()
    for key in global_dict.keys():
        client_params = torch.stack(
            [client.state_dict()[key].float() for client in client_models], dim=0
        )
        global_dict[key] = client_params.mean(dim=0).type(global_dict[key].dtype)
    global_model.load_state_dict(global_dict)
    return global_model

# Evaluate the model
def evaluate_model(model, dataset):
    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
    return 100 * correct / total

import copy

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize models
global_model = HybridModel().to(device)
client_models = [copy.deepcopy(global_model) for _ in range(10)]

# Training parameters
num_rounds = 10
local_epochs = 1
batch_size = 32
learning_rate = 0.01

# Federated learning
for round_num in range(num_rounds):
    print(f"Round {round_num + 1}")

    # Local training on clients
    for i, client_dataset in enumerate(client_data):
        print(f"Training on Client {i + 1}")
        train_local_model(client_dataset, client_models[i], local_epochs, batch_size, learning_rate)

    # Aggregation on the server
    global_model = federated_aggregate(global_model, client_models)

    # Broadcast the updated model to clients
    client_models = [copy.deepcopy(global_model) for _ in range(10)]

    # Evaluate global model
    accuracy = evaluate_model(global_model, test_dataset)
    print(f"Global Model Accuracy after Round {round_num + 1}: {accuracy:.2f}%")

# Save the best model
torch.save(global_model.state_dict(), "hybrid_model.pth")
print("Best model saved as 'hybrid_model.pth'")