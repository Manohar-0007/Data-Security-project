# -*- coding: utf-8 -*-
"""Level-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNKrI-Bfdk8ppRr_BkouBXonP7R1v6c0
"""

pip install torchvision

import torch
import torchvision
import torchvision.transforms as transforms
import numpy as np
from torch.utils.data import random_split, DataLoader
import torch.nn.functional as F
import torch.nn as nn
import copy

device = torch.device(cudaIdx if torch.cuda.is_available() else "cpu")

def load_data(transform, datasets='MNIST'):
    if datasets == 'MNIST':
        train_dataset = torchvision.datasets.MNIST(
            root="./data/mnist", train=True, download=True, transform=transform)
        test_dataset = torchvision.datasets.MNIST(
            root="./data/mnist", train=False, download=True, transform=transform)
    else:
        train_dataset = torchvision.datasets.CIFAR10(
            root="./data/cifar-10-python", train=True, download=True, transform=transform)
        test_dataset = torchvision.datasets.CIFAR10(
            root="./data/cifar-10-python", train=False, download=True, transform=transform)

    return train_dataset, test_dataset

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset, test_dataset = load_data(transform, datasets='MNIST')

print(train_dataset,test_dataset)

def split_dataset(dataset, num_clients=10):
    # Split dataset into `num_clients` partitions
    client_data = random_split(dataset, [len(dataset) // num_clients] * num_clients)
    return client_data

# Split the MNIST dataset into 10 parts
client_data = split_dataset(train_dataset, num_clients=10)

transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize to 224x224 for ResNet
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)),  # Normalize for single-channel images
    transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Repeat the single channel to create 3 channels
])

"""#Level-3 final"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
import numpy as np
import random
import copy
from torchvision import datasets, transforms
import torch.optim as optim

# Hybrid Model Definition (from Level 2)
class SimpleResNet(nn.Module):
    def __init__(self):
        super(SimpleResNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.layer1 = self._make_layer(16, 32, stride=2)
        self.layer2 = self._make_layer(32, 64, stride=2)

    def _make_layer(self, in_channels, out_channels, stride):
        layers = [
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
        ]
        return nn.Sequential(*layers)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        return x


class HybridModel(nn.Module):
    def __init__(self):
        super(HybridModel, self).__init__()
        self.feature_extractor = SimpleResNet()
        dummy_input = torch.randn(1, 1, 28, 28)
        feature_output = self.feature_extractor(dummy_input)
        self.feature_dim = feature_output.shape[1] * feature_output.shape[2] * feature_output.shape[3]
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )

    def forward(self, x):
        x = self.feature_extractor(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x


# Local training function
def train_local_model(dataset, model, epochs, batch_size, learning_rate):
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()

    model.train()
    for epoch in range(epochs):
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()


# Trimmed Mean Aggregation Function with Malicious Client Detection
def trimmed_mean_aggregate(global_model, client_models, round_num, malicious_client_id=None, trim_fraction=0.1):
    global_dict = global_model.state_dict()
    client_updates = []

    # Calculate updates from each client
    for client_model in client_models:
        update = {}
        for key in global_dict.keys():
            update[key] = client_model.state_dict()[key] - global_dict[key]
        client_updates.append(update)

    # Malicious client introduces noise if it's round >= 5
    if round_num >= 5 and malicious_client_id is not None:
        print(f"Malicious client {malicious_client_id + 1} introduced.")
        malicious_update = {
            key: torch.randn_like(value).float() if value.dtype in (torch.float32, torch.float64)
            else value for key, value in client_updates[malicious_client_id].items()
        }
        client_updates[malicious_client_id] = malicious_update

    # Aggregate updates with trimmed mean
    aggregated_update = {}
    num_clients = len(client_updates)
    num_trim = int(num_clients * trim_fraction)
    for key in global_dict.keys():
        stacked_updates = torch.stack([client[key].float() for client in client_updates])  # Ensure float type
        sorted_updates, _ = torch.sort(stacked_updates, dim=0)
        trimmed_updates = sorted_updates[num_trim:-num_trim]
        aggregated_update[key] = trimmed_updates.mean(dim=0)

    # Update global model
    for key in global_dict.keys():
        global_dict[key] = global_dict[key] + aggregated_update[key].to(global_dict[key].dtype)  # Match dtype

    global_model.load_state_dict(global_dict)
    return global_model


# Evaluate the model
def evaluate_model(model, dataset):
    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
    return 100 * correct / total


# Federated Learning Setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
global_model = HybridModel().to(device)
client_models = [copy.deepcopy(global_model) for _ in range(10)]

# Load MNIST dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
mnist_train = datasets.MNIST(root="./data", train=True, download=True, transform=transform)
mnist_test = datasets.MNIST(root="./data", train=False, download=True, transform=transform)

# Partition data for clients
client_data = torch.utils.data.random_split(mnist_train, [len(mnist_train) // 10] * 10)
test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)

# Training parameters
num_rounds = 5
local_epochs = 1
batch_size = 32
learning_rate = 0.01
malicious_client_index = random.randint(0, 9)
detection_round = 3

# Federated learning with attack detection
for round_num in range(num_rounds):
    print(f"Round {round_num + 1}")

    # Local training
    for i, client_dataset in enumerate(client_data):
        if i == malicious_client_index and round_num >= detection_round:
            # Malicious client sends random updates
            for param in client_models[i].parameters():
                param.data = torch.randn_like(param.data)
            print(f"Client {i} behaves maliciously!")
        else:
            train_local_model(client_dataset, client_models[i], local_epochs, batch_size, learning_rate)

    # Aggregation with attack detection
    global_model = trimmed_mean_aggregate(global_model, client_models, round_num, malicious_client_id=malicious_client_index)

    # Broadcast updated model
    client_models = [copy.deepcopy(global_model) for _ in range(10)]

    # Evaluate global model
    accuracy = evaluate_model(global_model, mnist_test)
    print(f"Global Model Accuracy after Round {round_num + 1}: {accuracy:.2f}%")

# Save the best model after training
torch.save(global_model.state_dict(), "Level3model.pth")
print("Best model saved as 'Level3model.pth'")

